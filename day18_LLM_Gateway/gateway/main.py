# gateway/main.py
import os
import time
import logging
from collections import defaultdict, deque
from typing import Dict, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv
from openai import OpenAI
# === Prometheus Metricsï¼ˆminimal setï¼‰ ===
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from services.retrieval_service import retrieve_best

REQS = Counter(
    "gateway_requests_total", "Total HTTP requests",
    labelnames=["route", "method", "status"]
)
LAT = Histogram(
    "gateway_request_duration_seconds", "Request latency (seconds)",
    labelnames=["route", "method"]
)
ERRS = Counter(
    "gateway_errors_total", "Errors by type",
    labelnames=["route", "type"]
)

# ============================================================
# ğŸŸ  Loggingï¼šåŒæ™‚è¼¸å‡º console + æª”æ¡ˆ gateway.log
# ============================================================
root_logger = logging.getLogger()
if not root_logger.handlers:
    root_logger.setLevel(logging.INFO)
    fmt = logging.Formatter("%(asctime)s %(levelname)s %(message)s")

    # Console Handler
    ch = logging.StreamHandler()
    ch.setFormatter(fmt)
    root_logger.addHandler(ch)

    # File Handler
    fh = logging.FileHandler("gateway.log", encoding="utf-8")
    fh.setFormatter(fmt)
    root_logger.addHandler(fh)

log = logging.getLogger(__name__)

# ============================================================
# ğŸŸ¢ FastAPI åˆå§‹åŒ– + CORS
# ============================================================
app = FastAPI(title="LLM Gateway (Day18)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # é–‹ç™¼å…ˆæ”¾å¯¬ï¼›ä¸Šç·šè«‹æ”¹ç™½åå–®
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# ğŸŸ¢ è®€å–ç’°å¢ƒè®Šæ•¸ / åˆå§‹åŒ– OpenAI
# ============================================================
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("æ²’æœ‰æ‰¾åˆ° OPENAI_API_KEYï¼Œè«‹åœ¨ .env æˆ–ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®šï¼")

OPENAI_TIMEOUT = float(os.getenv("OPENAI_TIMEOUT", "20"))
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")
client = OpenAI(timeout=OPENAI_TIMEOUT, max_retries=0)

# ============================================================
# ğŸŸ¢ å‰ç«¯é é¢ï¼ˆserve index.htmlï¼‰
# ============================================================
BACKEND_DIR = Path(__file__).resolve().parent
FRONTEND_DIR = BACKEND_DIR.parent / "frontend"

@app.get("/")
def root():
    idx = FRONTEND_DIR / "index.html"
    if idx.exists():
        return FileResponse(idx)
    return JSONResponse({"error": "frontend/index.html not found"}, status_code=404)

# è‹¥æœ‰ js/css/åœ–æª”ï¼Œå¯æ”¾åœ¨ /_static åº•ä¸‹
app.mount("/_static", StaticFiles(directory=FRONTEND_DIR, html=True), name="frontend")

# ============================================================
# ğŸŸ¢ å¥åº·æª¢æŸ¥
# ============================================================
@app.get("/healthz")
def healthz():
    return {"ok": True}

# ============================================================
# ğŸŸ¢ metrics 
# ç›®çš„ï¼š
# - æš´éœ² Prometheus å¯æŠ“å–çš„æŒ‡æ¨™ï¼ˆtext exposition formatï¼‰ã€‚
# - éƒ¨ç½²æ™‚åœ¨ Prometheus è¨­å®š scrape_configs æŒ‡å‘æ­¤ç«¯é»å³å¯ã€‚
# ============================================================
@app.get("/metrics")
def metrics():
    # CONTENT_TYPE_LATEST = "text/plain; version=0.0.4; charset=utf-8"
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

# ============================================================
# ğŸŸ¢ è¨˜æ†¶é«”é™æµ (In-Memory Rate Limit)
#   - Demo ç”¨ï¼šæ¯ IP æ¯åˆ†é˜æœ€å¤š 10 æ¬¡
#   - ç¨‹å¼é‡å•Ÿæœƒæ¸…ç©ºï¼›å¤š worker/å¤šæ©Ÿä¸å…±äº«ç‹€æ…‹ï¼›LB/æ°´å¹³æ“´å±•æ™‚ç„¡æ³•å…¨å±€ç”Ÿæ•ˆ
#   - ç”Ÿç”¢ç’°å¢ƒè«‹æ› Redis / é›²ç«¯ API Gateway
# ============================================================
_WINDOW_SEC = int(os.getenv("RATE_LIMIT_WINDOW_SEC", "60"))
_LIMIT = int(os.getenv("RATE_LIMIT_MAX_PER_IP", "10"))
_BUCKETS: dict[str, deque] = defaultdict(deque)

def in_memory_rate_limit(request: Request):
    ip = request.client.host
    now = time.time()
    dq = _BUCKETS[ip]

    # ç§»é™¤è¦–çª—å¤–çš„è«‹æ±‚
    while dq and now - dq[0] > _WINDOW_SEC:
        dq.popleft()

    if len(dq) >= _LIMIT:
        raise HTTPException(status_code=429, detail="Too Many Requests (demo limiter)")
    dq.append(now)

# ============================================================
# ğŸŸ¢ å°å·¥å…·ï¼šæª¢ç´¢ + LLM å›ç­”
# ============================================================
def handle_ask(question: str, temperature: float = 0.2) -> Dict[str, Any]:
    q = (question or "").strip()
    if not q:
        raise HTTPException(status_code=400, detail="question is required")

    # 1) æª¢ç´¢çŸ¥è­˜åº«
    context, dist = retrieve_best(q, k=3)

    # 2) å‘¼å« LLM
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ä¼æ¥­ FAQ åŠ©ç†ã€‚"},
        {"role": "user", "content": f"æ ¹æ“šä»¥ä¸‹çŸ¥è­˜åº«å…§å®¹å›ç­”ï¼š\n{context}\n\nå•é¡Œï¼š{q}"},
    ]
    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages,
        temperature=temperature,
    )
    answer = resp.choices[0].message.content

    return {
        "question": q,
        "context": context,
        "answer": answer,
        "debug": {"l2": dist, "model": CHAT_MODEL}
    }

# ============================================================
# ğŸŸ  /askï¼šPOST å°ˆç”¨ï¼ˆå«è¨˜æ†¶é«”é™æµ + ç°¡å–® Loggingï¼‰
#   - è¨˜éŒ„æ¯æ¬¡è«‹æ±‚çš„ä¾†æºèˆ‡å•é¡Œ
#   - æˆåŠŸèˆ‡å¤±æ•—éƒ½æœƒ log
# ============================================================
@app.post("/ask")
def ask(payload: Dict[str, Any], request: Request):
    # ğŸŸ¢ é™æµæª¢æŸ¥
    in_memory_rate_limit(request)

    q = (payload.get("question") or "").strip()
    temp = float(payload.get("temperature", 0.2))

    # ğŸŸ  Loggingï¼šè¨˜éŒ„ä¾†æºèˆ‡å•é¡Œ
    log.info(f"[ASK] ip={request.client.host} q={q!r} temp={temp}")

    start = time.time()
    try:
        result = handle_ask(q, temp)
        log.info(f"[ASK] success in {time.time()-start:.2f}s, answer_len={len(result['answer'])}")
        return result
    except HTTPException:
        log.exception("[ASK] http error")
        raise
    except Exception:
        log.exception("[ASK] failed")
        return JSONResponse(status_code=500, content={"error": "internal error"})

# ============================================================
# Metrics Middleware
# ç›®çš„ï¼š
# - çµ±ä¸€é‡æ¸¬æ‰€æœ‰ API çš„è«‹æ±‚æ•¸ã€å»¶é²ï¼ˆp50/p95 å¯ç”± Prometheus/Grafana è¨ˆç®—ï¼‰ã€éŒ¯èª¤é¡å‹ã€‚
# - ä¸éœ€è¦æ¯å€‹è·¯ç”±éƒ½æ‰‹åˆ»è¨ˆæ•¸é‚è¼¯ã€‚
# ============================================================
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    route = request.url.path or "unknown"
    method = request.method

    # ä¸è¦é‡æ¸¬ /metrics æœ¬èº«ï¼Œé¿å…è‡ªæˆ‘å¹²æ“¾
    if route == "/metrics":
        return await call_next(request)

    with LAT.labels(route, method).time():
        status = "500"
        try:
            resp = await call_next(request)
            status = str(resp.status_code)
            return resp
        except HTTPException as he:
            status = str(he.status_code)
            ERRS.labels(route, "http").inc()
            raise
        except Exception:
            ERRS.labels(route, "unhandled").inc()
            raise
        finally:
            REQS.labels(route, method, status).inc()