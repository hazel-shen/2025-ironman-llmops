# pipeline.py
from __future__ import annotations
import os
from steps.detect import detect_change, save_hash
from steps.embed import embed_texts
from steps.update_index import write_index
from dotenv import load_dotenv


SRC_FILE = "data/faq.txt"
META_HASH = "artifacts/source.hash"
INDEX_OUT = "artifacts/vector_index.json"

load_dotenv()

def simple_chunk(text: str, max_chars: int = 400) -> list[str]:
    # æ¥µç°¡åˆ‡ç‰‡ï¼šä¾æ®µè½èˆ‡é•·åº¦åˆ‡
    parts = []
    for para in text.splitlines():
        if not para.strip():
            continue
        buf = para.strip()
        while len(buf) > max_chars:
            parts.append(buf[:max_chars])
            buf = buf[max_chars:]
        if buf:
            parts.append(buf)
    return parts or ["(ç©ºæ–‡ä»¶)"]

def main() -> None:
    print("ğŸ” æª¢æŸ¥æª”æ¡ˆæ˜¯å¦è®Šå‹•â€¦")
    res = detect_change(SRC_FILE, META_HASH)
    if not res.changed:
        print("âœ… ç„¡è®Šå‹•ï¼Œè·³éæ›´æ–°")
        return

    print("âš ï¸ åµæ¸¬åˆ°è®Šå‹•ï¼Œé–‹å§‹æ›´æ–°ç´¢å¼•â€¦")
    with open(SRC_FILE, "r", encoding="utf-8") as f:
        text = f.read()

    chunks = simple_chunk(text)
    print(f"âœ‚ï¸ åˆ‡å‡º {len(chunks)} å€‹ç‰‡æ®µ")

    vectors = embed_texts(chunks)
    print(f"ğŸ§  ç”¢ç”Ÿ {len(vectors)} ç­†å‘é‡")

    records = [
        {"id": i, "text": chunks[i], "vector": vectors[i]}
        for i in range(len(chunks))
    ]
    write_index(INDEX_OUT, records)
    save_hash(META_HASH, res.new_hash)
    print(f"ğŸ“¦ å·²å¯«å…¥ç´¢å¼•ï¼š{INDEX_OUT}")
    print("ğŸ‰ æ›´æ–°å®Œæˆï¼")

if __name__ == "__main__":
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs("data", exist_ok=True)
    os.makedirs("artifacts", exist_ok=True)
    if not os.path.exists(SRC_FILE):
        with open(SRC_FILE, "w", encoding="utf-8") as f:
            f.write("å…¬å¸å“¡å·¥æ‰‹å†Š v1.0ï¼š\nç¬¬äºŒç«  åŠ ç­èˆ‡è£œä¼‘ï¼šåŠ ç­éœ€äº‹å‰ç”³è«‹ï¼Œå·¥æ™‚å¯æŠ˜æ›è£œä¼‘ã€‚\n")
    main()
