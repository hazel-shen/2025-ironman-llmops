# scripts/add_docs.py
import os, json, uuid, argparse, hashlib, sys
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

DATA_DIR = "data"
INDEX_PATH = os.path.join(DATA_DIR, "kb.index")
KB_PATH = os.path.join(DATA_DIR, "kb.jsonl")
MAP_PATH = os.path.join(DATA_DIR, "mappings.json")
META_PATH = os.path.join(DATA_DIR, "kb_meta.json")

DIM = 384
MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"


def norm_text(s: str) -> str:
    # æœ€ç°¡ normalizationï¼šå»é¦–å°¾ç©ºç™½ã€å£“ç¸®é€£çºŒç©ºç™½
    return " ".join(s.strip().split())


def text_hash(s: str) -> str:
    return hashlib.sha256(norm_text(s).encode("utf-8")).hexdigest()


def load_index():
    if os.path.exists(INDEX_PATH):
        print("ğŸ—‚ï¸  è¼‰å…¥æ—¢æœ‰ç´¢å¼• ...")
        return faiss.read_index(INDEX_PATH)
    print("ğŸ†•  å»ºç«‹æ–°ç´¢å¼• ...")
    return faiss.IndexFlatL2(DIM)


def load_existing_hashes():
    hashes = set()
    if os.path.exists(KB_PATH):
        with open(KB_PATH, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                o = json.loads(line)
                h = o.get("text_hash") or text_hash(o["text"])
                hashes.add(h)
    return hashes


def rebuild_rowid_mapping():
    row_map = {}
    with open(KB_PATH, "r", encoding="utf-8") as f:
        for idx, line in enumerate(f):
            if not line.strip():
                continue
            o = json.loads(line)
            row_map[str(idx)] = o["doc_id"]
    return row_map


def save_json(path, obj):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def main(args):
    # æ”¶é›†è¼¸å…¥æ–‡æœ¬
    new_texts = []
    if args.text:
        new_texts.extend(args.text)
    if args.jsonl:
        with open(args.jsonl, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                o = json.loads(line)
                t = o["text"] if isinstance(o, dict) else str(o)
                new_texts.append(t)

    # è‹¥æ²’æä¾›ï¼Œé è¨­ç¤ºä¾‹ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è©¦è·‘æ™‚æ–¹ä¾¿ï¼‰
    if not new_texts:
        new_texts = [
            "2025 å¹´ VPN è¨­å®šæµç¨‹ï¼šæ­¥é©Ÿ 1 ä¸‹è¼‰æ–°ç‰ˆå®¢æˆ¶ç«¯ï¼Œæ­¥é©Ÿ 2 ä½¿ç”¨ SSO ç™»å…¥ã€‚",
            "æ–°çš„äººè³‡æ”¿ç­–ï¼šè©¦ç”¨æœŸæ»¿å¾Œæ–¹å¯ç”³è«‹é ç«¯è¾¦å…¬ã€‚",
        ]
        print("â„¹ï¸ æœªæŒ‡å®šè¼¸å…¥ï¼Œä½¿ç”¨é è¨­ç¤ºä¾‹å…©ç­†ã€‚ä¸‹æ¬¡å¯ç”¨ -t/--text æˆ– --jsonl æŒ‡å®šã€‚")

    # å»é‡ï¼šæ¯”å°æ—¢æœ‰é›œæ¹Š
    existing = load_existing_hashes()
    to_add = []
    skipped = 0
    for t in new_texts:
        h = text_hash(t)
        if h in existing:
            skipped += 1
            continue
        to_add.append((t, h))
        existing.add(h)

    if not to_add:
        print(f"âœ… ç„¡éœ€æ–°å¢ï¼šå…¨éƒ¨ {len(new_texts)} ç­†çš†ç‚ºé‡è¤‡ï¼ˆå·²å­˜åœ¨ï¼‰ã€‚")
        return

    # 1) ç´¢å¼•
    index = load_index()

    # 2) å‘é‡åŒ–
    print(f"ğŸ§   è¼‰å…¥åµŒå…¥æ¨¡å‹ï¼š{MODEL_ID}")
    model = SentenceTransformer(MODEL_ID)
    texts_only = [t for t, _ in to_add]
    print(f"ğŸ”¢  å‘é‡åŒ– {len(texts_only)} ç­†æ–‡æœ¬ ...")
    vecs = model.encode(texts_only, normalize_embeddings=True, show_progress_bar=True)
    vecs = np.asarray(vecs, dtype="float32")

    # 3) å¯«å…¥ç´¢å¼•
    print("ğŸ“Œ  å¯«å…¥ç´¢å¼• ...")
    index.add(vecs)
    faiss.write_index(index, INDEX_PATH)

    # 4) è¿½åŠ åŸæ–‡ï¼ˆå« text_hashï¼‰
    print("ğŸ“  è¿½åŠ åŸæ–‡ ...")
    with open(KB_PATH, "a", encoding="utf-8") as f:
        for t, h in to_add:
            doc_id = f"doc_{uuid.uuid4().hex[:8]}"
            f.write(json.dumps({"doc_id": doc_id, "text": t, "text_hash": h}, ensure_ascii=False) + "\n")

    # 5) æ›´æ–°æ˜ å°„
    print("ğŸ”—  æ›´æ–° rowid å°æ‡‰ ...")
    mappings = {"rowid_to_doc_id": rebuild_rowid_mapping()}
    save_json(MAP_PATH, mappings)

    # 6) æ›´æ–°ä¸­ç¹¼è³‡æ–™
    print("ğŸ§¾  æ›´æ–° KB meta ...")
    meta = json.load(open(META_PATH, encoding="utf-8"))
    meta["kb_version"] += 1
    meta["doc_count"] = len(mappings["rowid_to_doc_id"])
    save_json(META_PATH, meta)

    print(f"âœ… å®Œæˆï¼šæ–°å¢ {len(to_add)} ç­†ï¼›è·³éé‡è¤‡ {skipped} ç­†ï¼›kb_version={meta['kb_version']}ï¼›ç¸½ç­†æ•¸={meta['doc_count']}")


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("-t", "--text", action="append", help="ç›´æ¥æ–°å¢ä¸€æ®µæ–‡æœ¬ï¼›å¯é‡è¤‡æŒ‡å®šå¤šæ¬¡")
    ap.add_argument("--jsonl", type=str, help="å¾ jsonl æª”æ–°å¢ï¼Œæ ¼å¼ï¼šæ¯è¡Œä¸€å€‹ç‰©ä»¶ï¼Œè‡³å°‘å« text æ¬„ä½")
    args = ap.parse_args()
    main(args)
