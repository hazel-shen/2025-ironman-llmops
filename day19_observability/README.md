# ğŸ“Š LLM Observability Demo (FastAPI + Prometheus + Grafana)

é€™æ˜¯ä¸€å€‹æœ€å°å¯è¡Œçš„ LLM Observability ç¯„ä¾‹å°ˆæ¡ˆï¼Œé€é FastAPI åŒ…è£ OpenAIï¼Œä¸¦è¼¸å‡º Prometheus Metricsï¼Œæœ€å¾Œç”¨ Grafana Dashboard è¦–è¦ºåŒ–ã€‚

## ğŸš€ åŠŸèƒ½

`/ask`ï¼šå‘¼å« OpenAI APIï¼ˆçœŸå¯¦åŸ·è¡Œï¼‰ï¼Œä¸¦å›å‚³ Latencyã€Token ä½¿ç”¨é‡èˆ‡æˆæœ¬
`/metrics`ï¼šPrometheus exporterï¼Œè¼¸å‡º LLM ç›¸é—œ metrics

é è¨­æä¾›çš„ metricsï¼š

- `llm_requests_total`ï¼šè«‹æ±‚æ•¸é‡
- `llm_tokens_total`ï¼šToken ä½¿ç”¨é‡ï¼ˆprompt/completion åˆ†é–‹ï¼‰
- `llm_request_latency_seconds`ï¼šå»¶é²ç›´æ–¹åœ–ï¼ˆå¯ç®— P95/P99ï¼‰
- `llm_cost_usd_total`ï¼šç´¯ç©æˆæœ¬ï¼ˆUSDï¼‰
- `llm_errors_total`ï¼šéŒ¯èª¤çµ±è¨ˆ

## ğŸ“‚ å°ˆæ¡ˆçµæ§‹

```yaml
day19_observability/
â”œâ”€ app/
â”‚  â”œâ”€ app.py              # FastAPI + Prometheus exporter
â”‚  â”œâ”€ requirements.txt
â”‚  â””â”€ Dockerfile
â”œâ”€ tests/
â”‚  â””â”€ test_requests.py    # æ¸¬è©¦è…³æœ¬
â”œâ”€ docker/
â”‚  â”œâ”€ prometheus.yml
â”‚  â””â”€ grafana/
â”‚     â””â”€ provisioning/
â”‚        â”œâ”€ dashboards/
â”‚        â”‚  â”œâ”€ dashboard.yml
â”‚        â”‚  â””â”€ llm_observability.json
â”‚        â””â”€ datasources/
â”‚           â””â”€ datasource.yml
â”œâ”€ environment.yaml        # Conda ç’°å¢ƒè¨­å®š
â”œâ”€ .env                   # ç’°å¢ƒè®Šæ•¸ (ä¸è¦ä¸Šå‚³)
â”œâ”€ .env.example           # ç’°å¢ƒè®Šæ•¸ç¯„ä¾‹
â”œâ”€ docker-compose.yml
â”œâ”€ README.md
â””â”€ .dockerignore
```

âš™ï¸ ç’°å¢ƒè®Šæ•¸è¨­å®šï¼ˆ.envï¼‰

```env
APP_PORT=8000
PROM_PORT=9090
GRAFANA_PORT=3000

GRAFANA_USER=admin
GRAFANA_PASS=admin

# OpenAI API
OPENAI_API_KEY=ä½ çš„_API_KEY
OPENAI_MODEL=gpt-4o-mini
OPENAI_TIMEOUT=30
```

âš ï¸ è«‹ç¢ºä¿ .env æ²’æœ‰è¢« commit ä¸Š GitHubï¼ˆå·²å»ºè­°åœ¨ .gitignore å¿½ç•¥ï¼‰ã€‚

## â–¶ï¸ å…©ç¨®ä½¿ç”¨æ¨¡å¼

| æ¨¡å¼                     | æŠ€è¡“                                            | é©åˆç”¨é€”                   | æ˜¯å¦å¯çœ‹ Grafana            |
| ------------------------ | ----------------------------------------------- | -------------------------- | --------------------------- |
| **æ–¹æ³•ä¸€ï¼šé–‹ç™¼æ¨¡å¼**     | Conda + FastAPI                                 | æœ¬åœ°æ¸¬è©¦ APIã€Debug        | âŒ åªèƒ½çœ‹ `/metrics` ç´”æ–‡å­— |
| **æ–¹æ³•äºŒï¼šå®Œæ•´è§€æ¸¬æ¨¡å¼** | Docker Compose (FastAPI + Prometheus + Grafana) | å®Œæ•´ç›£æ§ã€è¦–è¦ºåŒ– Dashboard | âœ…                          |

### æ–¹æ³•ä¸€ï¼šé–‹ç™¼æ¨¡å¼ï¼ˆCondaï¼Œæœ¬æ©Ÿæ¸¬è©¦ app.py ç”¨çš„ï¼‰

1. å»ºç«‹ç’°å¢ƒä¸¦å•Ÿç”¨

```bash
cd day19_observability
conda env create -f environment.yaml
conda activate day19_observability
```

2. ç¢ºä¿ `.env` å·²è¨­å®šå¥½ï¼ˆè‡³å°‘è¦æœ‰ `OPENAI_API_KEY`ï¼‰ã€‚

3. æ¸¬è©¦ APIï¼š

```bash
curl -X POST http://localhost:8000/ask \
  -H 'Content-Type: application/json' \
  -d '{"model":"gpt-4o-mini","messages":[{"role":"user","content":"ç”¨ä¸‰é»è§£é‡‹ RAG"}]}'
```

4. æŸ¥çœ‹ metricsï¼š

```bash
curl http://localhost:8000/metrics | head -50
```

æ¸¬è©¦å®Œä¸éœ€è¦ç”¨åˆ°çš„æ™‚å€™ï¼š

```bash
conda remove --name day19_observability --all
```

> âš ï¸ æ³¨æ„ï¼šé€™æœƒåˆªé™¤æ•´å€‹ç’°å¢ƒèˆ‡å®‰è£çš„å¥—ä»¶ã€‚

### æ–¹æ³•äºŒï¼šå®Œæ•´è§€æ¸¬æ¨¡å¼ï¼ˆDocker Composeï¼‰

1. å»ºç½®ä¸¦å•Ÿå‹•å®¹å™¨

> é©åˆå®Œæ•´è§€æ¸¬ï¼ŒåŒ…æ‹¬ Latency P95ã€Token æ¶ˆè€—è¶¨å‹¢ã€æˆæœ¬èµ°å‹¢ç­‰ã€‚

```bash
docker compose up -d --build
```

2. æ¸¬è©¦ API

```bash
curl -X POST http://localhost:8000/ask \
  -H 'Content-Type: application/json' \
  -d '{"model":"gpt-4o-mini","messages":[{"role":"user","content":"ç”¨ä¸‰é»è§£é‡‹ RAG"}]}'
```

ç¯„ä¾‹å›æ‡‰ï¼š

```json
{
  "model": "gpt-4o-mini",
  "latency_s": 0.85,
  "prompt_tokens": 120,
  "completion_tokens": 180,
  "cost_usd": 0.00021,
  "answer": "RAG æ˜¯ Retrieval-Augmented Generation..."
}
```

3. æŸ¥çœ‹ Metrics

```bash
curl http://localhost:8000/metrics
```

ç¯„ä¾‹è¼¸å‡ºï¼š

```bash
# HELP llm_requests_total Total LLM requests
# TYPE llm_requests_total counter
llm_requests_total{model="gpt-4o-mini"} 9.0
llm_requests_total{model="gpt-4o"} 2.0
llm_requests_total{model="gpt-4.1"} 1.0
# HELP llm_tokens_total Total tokens used
# TYPE llm_tokens_total counter
llm_tokens_total{model="gpt-4o-mini",type="prompt"} 265.0
llm_tokens_total{model="gpt-4o-mini",type="completion"} 435.0
llm_tokens_total{model="gpt-4o",type="prompt"} 57.0
llm_tokens_total{model="gpt-4o",type="completion"} 641.0
llm_tokens_total{model="gpt-4.1",type="prompt"} 28.0
```

4. é€²å…¥ä»‹é¢

- Prometheus: http://localhost:9090
- Grafana: http://localhost:3000
  ï¼ˆå¸³è™Ÿå¯†ç¢¼è¦‹ .envï¼‰

> âš ï¸ Prometheus æœƒå®šæœŸæŠ“ /metricsï¼ŒGrafana æœƒè‡ªå‹•è¼‰å…¥ Dashboardï¼Œèƒ½çœ‹åˆ° Latency / Tokens / Cost æ›²ç·šã€‚

5. åœæ­¢ä½†ä¿ç•™å®¹å™¨ï¼š

```bash
docker compose stop
```

6. åœæ­¢ä¸¦åˆªé™¤å®¹å™¨èˆ‡ networkï¼š

```bash
docker compose down
```

7. è‹¥è¦é€£åŒ volume ä¸€èµ·åˆªé™¤ï¼ˆâš ï¸ è³‡æ–™æœƒæ¶ˆå¤±ï¼‰ï¼š

```bash
docker compose down -v
```

---

## ğŸ“Š Grafana Dashboard

å°ˆæ¡ˆå…§å·²æä¾›ç°¡å–®çš„ Dashboard JSONï¼š

- Requests ç¸½æ•¸
- Latency P95 æ›²ç·š
- Token ä½¿ç”¨é‡
- æˆæœ¬èµ°å‹¢

åŒ¯å…¥å¾Œå³å¯å¿«é€ŸæŸ¥çœ‹ã€‚

## ğŸ§ª æ¸¬è©¦è…³æœ¬

å°ˆæ¡ˆå…§æœ‰ `tests/test_requests.py`ï¼Œæœƒæ‰“å¹¾å€‹ API ä¸¦é¡¯ç¤º metrics è®Šå‹•ã€‚

åŸ·è¡Œæ–¹å¼ï¼š

```bash
python tests/test_requests.py
```

ç¯„ä¾‹è¼¸å‡ºï¼š

```bash
âœ… gpt-4o-mini å›ç­”ï¼šRAG æ˜¯ä¸€ç¨®æª¢ç´¢å¢å¼·ç”ŸæˆæŠ€è¡“... (tokens=15+120, cost=0.00013$)
âœ… gpt-4o-mini å›ç­”ï¼šRAG stands for Retrieval... (tokens=12+80, cost=0.00008$)
âœ… gpt-4o å›ç­”ï¼šåœ¨é‡‘èç”¢æ¥­ï¼ŒRAG å¯ä»¥ç”¨æ–¼... (tokens=30+200, cost=0.0026$)

ğŸ“Š Metrics éƒ¨åˆ†è¼¸å‡ºï¼š
llm_requests_total{model="gpt-4o-mini"} 2.0
llm_requests_total{model="gpt-4o"} 1.0
llm_tokens_total{model="gpt-4o-mini",type="prompt"} 27.0
llm_tokens_total{model="gpt-4o-mini",type="completion"} 200.0
llm_cost_usd_total{model="gpt-4o-mini"} 0.00021
llm_cost_usd_total{model="gpt-4o"} 0.0026
```

## ğŸ› ï¸ å¾ŒçºŒæ“´å……

- å¢åŠ æ›´å¤šæ¨¡å‹å®šåƒ¹ï¼ˆç›®å‰æ”¯æ´ GPT-4oã€GPT-4o-miniï¼Œå¯ä¾éœ€æ±‚æ“´å……ï¼‰
- åŠ å…¥ Rate Limit/429 éŒ¯èª¤ç›£æ§
- èª¿æ•´ Latency Histogram buckets ä»¥ç¬¦åˆå¯¦éš› SLA
- æ¥å…¥ Node Exporter ç›£æ§ CPU/Mem
