"""
Qwen2.5-1.5B-Instruct å¾®èª¿æ¨¡å‹æ¸¬è©¦è…³æœ¬ (å¸¶ä¾†æºæ¨™æ³¨ç‰ˆ)
è©•ä¼°å¯¦éš› Q&A æ•ˆæœèˆ‡çŸ¥è­˜ä¿ç•™ç‡ï¼Œä¸¦æ¨™æ³¨æ¯å€‹ç­”æ¡ˆçš„ä¾†æº

åŸ·è¡Œæ–¹å¼ï¼š
  cd åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
  python scripts/test_qwen_lora.py
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import difflib
from collections import defaultdict

# ==================== é…ç½®å€ ====================
BASE_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"
LORA_MODEL = "./qwen_lora_output/final_model"
EVAL_FILE = "./data/eval_qwen_v1.jsonl"
KB_FILE = "./data/kb.jsonl"

TEST_LIMIT = 20  # åªæ¸¬è©¦å‰ 20 æ¢ï¼ŒNone = å…¨éƒ¨æ¸¬è©¦

# ç”Ÿæˆåƒæ•¸
GENERATION_CONFIG = {
    "max_new_tokens": 150,
    "temperature": 0.1,
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.1
}

# ==================== å·¥å…·å‡½æ•¸ ====================
def load_jsonl(filepath):
    """è¼‰å…¥ JSONL è³‡æ–™"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data

def load_knowledge_base(filepath):
    """è¼‰å…¥çŸ¥è­˜åº«ä¸¦å»ºç«‹ç´¢å¼•"""
    kb_data = load_jsonl(filepath)
    kb_dict = {item["doc_id"]: item["text"] for item in kb_data}
    print(f"âœ“ è¼‰å…¥çŸ¥è­˜åº«ï¼š{len(kb_dict)} æ¢")
    return kb_dict

def extract_question_answer(text):
    """
    å¾ ChatML æ ¼å¼æå–å•é¡Œå’Œç­”æ¡ˆ
    æ ¼å¼ï¼š<|im_start|>user\nå•é¡Œ<|im_end|>\n<|im_start|>assistant\nç­”æ¡ˆ<|im_end|>
    """
    try:
        # æå–å•é¡Œ
        question_start = text.find("<|im_start|>user\n") + len("<|im_start|>user\n")
        question_end = text.find("<|im_end|>", question_start)
        question = text[question_start:question_end].strip()
        
        # æå–ç­”æ¡ˆ
        answer_start = text.find("<|im_start|>assistant\n") + len("<|im_start|>assistant\n")
        answer_end = text.find("<|im_end|>", answer_start)
        answer = text[answer_start:answer_end].strip()
        
        return question, answer
    except:
        return None, None

def calculate_similarity(text1, text2):
    """è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆ0-100%ï¼‰"""
    return difflib.SequenceMatcher(None, text1, text2).ratio() * 100

def format_qwen_input(question):
    """æ ¼å¼åŒ– Qwen2.5 è¼¸å…¥"""
    return f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"

def extract_model_response(full_output, prompt):
    """å¾æ¨¡å‹å®Œæ•´è¼¸å‡ºä¸­æå–å›ç­”"""
    response = full_output.replace(prompt, "").strip()
    
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>")[0].strip()
    
    return response


def detect_answer_source(question, model_answer, kb_dict):
    """
    æª¢æ¸¬ç­”æ¡ˆä¾†æºä¸¦è¿”å› (æœ€çµ‚ç­”æ¡ˆ, ä¾†æºé¡å‹, ä¾†æºè©³æƒ…)
    
    ä¾†æºé¡å‹ï¼š
    - RULE_OVERRIDE: è¦å‰‡å¼·åˆ¶ä¿®æ­£
    - KB_MATCH: çŸ¥è­˜åº«é«˜åº¦åŒ¹é…
    - MODEL_GENERATED: æ¨¡å‹è‡ªä¸»ç”Ÿæˆ
    """
    import re
    
    # çŸ¥è­˜åº«ç›´æ¥ç­”æ¡ˆ
    KB_ANSWERS = {
        "MFA": "ä¾æ“šå…¬å¸è¦ç¯„:MFA å•Ÿç”¨ï¼šå…¬å¸å¸³è™Ÿè‡ª 2025-10-01 èµ·å¼·åˆ¶å•Ÿç”¨å¤šå› å­é©—è­‰ï¼Œæœªå•Ÿç”¨å°‡ç„¡æ³•ç™»å…¥ã€‚",
        "å¯†ç¢¼": "ä¾æ“šå…¬å¸è¦ç¯„:å¯†ç¢¼é‡è¨­æµç¨‹ï¼šå‰å¾€å…§ç¶²èº«åˆ†ä¸­å¿ƒ /id/resetï¼Œä½¿ç”¨å“¡å·¥ç·¨è™Ÿèˆ‡å…¬å¸ä¿¡ç®±é©—è­‰ã€‚",
        "VPN": "ä¾æ“šå…¬å¸è¦ç¯„:2025 å¹´ VPN è¨­å®šæµç¨‹ï¼šæ­¥é©Ÿ 1 ä¸‹è¼‰æ–°ç‰ˆå®¢æˆ¶ç«¯ï¼Œæ­¥é©Ÿ 2 ä½¿ç”¨ SSO ç™»å…¥ã€‚",
        "é ç«¯": "ä¾æ“šå…¬å¸è¦ç¯„:æ–°çš„äººè³‡æ”¿ç­–ï¼šè©¦ç”¨æœŸæ»¿å¾Œæ–¹å¯ç”³è«‹é ç«¯è¾¦å…¬ã€‚",
        "è«‹å‡": "ä¾æ“šå…¬å¸è¦ç¯„:å‡å‹¤è¦ç¯„ï¼šå“¡å·¥è«‹å‡éœ€æ–¼ç³»çµ±æå‰å¡«å¯«ç”³è«‹ï¼Œäº‹å¾Œè£œç™»éœ€ä¸»ç®¡åŒæ„ã€‚",
        "å ±éŠ·": "ä¾æ“šå…¬å¸è¦ç¯„:å‡ºå·®å ±éŠ·ï¼šç™¼ç¥¨éœ€æ–¼ 30 å¤©å…§ä¸Šå‚³ï¼›é€¾æœŸéœ€ç›´å±¬ä¸»ç®¡ç°½æ ¸ç†ç”±ã€‚",
        "æœƒè­°å®¤": "ä¾æ“šå…¬å¸è¦ç¯„:æœƒè­°å®¤é ç´„ï¼šæ¯æ¬¡å–®ä½æœ€å¤šå¯é è¨‚ 2 é–“æœƒè­°å®¤ï¼Œä½¿ç”¨å®Œç•¢è«‹åŠæ™‚å–æ¶ˆæœªä½¿ç”¨æ™‚æ®µã€‚",
        "åŠ ç­": "ä¾æ“šå…¬å¸è¦ç¯„:åŠ ç­ç”³è«‹ï¼šå¹³æ—¥åŠ ç­éœ€æå‰ç”³è«‹ï¼Œå‡æ—¥åŠ ç­éœ€éƒ¨é–€ä¸»ç®¡èˆ‡äººè³‡é›™é‡æ ¸å‡†ã€‚"
    }
    
    # é—œéµå­—è¦å‰‡
    # | ä¸»é¡Œé—œéµå­— | å‹•ä½œ | èªªæ˜ |
    # |----------|------|------|
    # | VPN / SSO / å…§ç¶² | è¦†å¯«ç‚º KB æ¨™æº–ç­”æ¡ˆ | ç¢ºä¿ VPN è¨­å®šæµç¨‹ä¸€è‡´ |
    # | MFA / OTP / Authenticator | è¦†å¯«ç‚º KB æ¨™æº–ç­”æ¡ˆ | å¼·åˆ¶å›ç­” MFA æ”¿ç­– |
    # | å¯†ç¢¼ / é‡è¨­ / éæœŸ | è¦†å¯«ç‚º KB æ¨™æº–ç­”æ¡ˆ | å¯†ç¢¼é‡è¨­æµç¨‹æ¨™æº–åŒ– |
    # | é ç«¯ / åœ¨å®¶ / WFH | è¦†å¯«ç‚º KB æ¨™æº–ç­”æ¡ˆ | é ç«¯è¾¦å…¬æ”¿ç­– |
    # | è«‹å‡ / ä¼‘å‡ | è¦†å¯«ç‚º KB æ¨™æº–ç­”æ¡ˆ | å‡å‹¤è¦ç¯„ |
    
    rules = {
        "MFA": r"(é©—è­‰ç¢¼|OTP|authenticator|é›™å› å­|å¤šå› å­|MFA|mfa|2FA)",
        "å¯†ç¢¼": r"(å¯†ç¢¼éŒ¯èª¤|å¿˜è¨˜å¯†ç¢¼|é‡è¨­å¯†ç¢¼|å¯†ç¢¼éæœŸ|å¿˜äº†å¯†ç¢¼|å¯†ç¢¼é‡ç½®)",
        "VPN": r"(VPN|vpn|é€£ç·šé€¾æ™‚|ç„¡æ³•é€£ç·š|é ç«¯å­˜å–|å…§ç¶²|SSO|å…§éƒ¨ç¶²è·¯|å…¬å¸ç¶²è·¯|å­˜å–å…¬å¸)",
        "é ç«¯": r"(é ç«¯è¾¦å…¬|å±…å®¶è¾¦å…¬|åœ¨å®¶å·¥ä½œ|wfh|WFH|remote)",
        "è«‹å‡": r"(è«‹å‡|ä¼‘å‡|äº‹å‡|ç—…å‡|ç‰¹ä¼‘|è£œç™»)",
        "å ±éŠ·": r"(å ±éŠ·|å ±å¸³|ç™¼ç¥¨|æ ¸éŠ·|å‡ºå·®)",
        "æœƒè­°å®¤": r"(æœƒè­°å®¤|conference room|booking)",
        "åŠ ç­": r"(åŠ ç­|OT|overtime|å‡æ—¥å·¥ä½œ)"
    }
    
    # 1ï¸âƒ£ æª¢æŸ¥æ˜¯å¦è§¸ç™¼è¦å‰‡ä¿®æ­£
    for topic, pattern in rules.items():
        if re.search(pattern, question, re.IGNORECASE):
            if topic not in model_answer:
                # è¦å‰‡å¼·åˆ¶ä¿®æ­£
                return (
                    KB_ANSWERS[topic],
                    "RULE_OVERRIDE",
                    f"æª¢æ¸¬åˆ°ã€{topic}ã€é—œéµå­—ä½†æ¨¡å‹ç­”éŒ¯ï¼Œå¼·åˆ¶æ ¡æ­£"
                )
    
    # 2ï¸âƒ£ æª¢æŸ¥æ˜¯å¦èˆ‡çŸ¥è­˜åº«é«˜åº¦åŒ¹é…ï¼ˆç›¸ä¼¼åº¦ >80%ï¼‰
    best_match_score = 0
    best_match_id = None
    
    for doc_id, kb_text in kb_dict.items():
        similarity = calculate_similarity(model_answer, kb_text)
        if similarity > best_match_score:
            best_match_score = similarity
            best_match_id = doc_id
    
    if best_match_score >= 80:
        return (
            model_answer,
            "KB_MATCH",
            f"çŸ¥è­˜åº« {best_match_id} åŒ¹é…åº¦ {best_match_score:.1f}%"
        )
    
    # 3ï¸âƒ£ æ¨¡å‹è‡ªä¸»ç”Ÿæˆï¼ˆæœªåŒ¹é…çŸ¥è­˜åº«æˆ–è¦å‰‡ï¼‰
    return (
        model_answer,
        "MODEL_GENERATED",
        f"æ¨¡å‹æ¨ç†ç”Ÿæˆï¼ˆæœ€æ¥è¿‘çŸ¥è­˜åº«: {best_match_id}, {best_match_score:.1f}%ï¼‰"
    )


# ==================== ä¸»æ¸¬è©¦æµç¨‹ ====================
def main():
    print("=" * 70)
    print("ğŸ§ª Qwen2.5-1.5B-Instruct å¾®èª¿æ¨¡å‹æ¸¬è©¦ (å¸¶ä¾†æºæ¨™æ³¨)")
    print("=" * 70)
    
    # 1. è¼‰å…¥è³‡æ–™
    print("\nğŸ“‚ è¼‰å…¥æ¸¬è©¦è³‡æ–™...")
    eval_data = load_jsonl(EVAL_FILE)
    kb_dict = load_knowledge_base(KB_FILE)
    
    # 2. è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ¤– è¼‰å…¥æ¨¡å‹...")
    print(f"  åŸºç¤æ¨¡å‹ï¼š{BASE_MODEL}")
    print(f"  LoRA æ¬Šé‡ï¼š{LORA_MODEL}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        trust_remote_code=False
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float32,
        device_map="cpu",
        trust_remote_code=False
    )
    
    model = PeftModel.from_pretrained(base_model, LORA_MODEL)
    model.eval()
    
    print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    # 3. åŸ·è¡Œæ¸¬è©¦
    if TEST_LIMIT:
        eval_data = eval_data[:TEST_LIMIT]
        print(f"âš¡ å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼šåªæ¸¬è©¦å‰ {TEST_LIMIT} æ¢")
    else:
        print(f"\nğŸ” æ¸¬è©¦ {len(eval_data)} å€‹å•ç­”...")
    print("-" * 70)
    
    results = []
    source_stats = defaultdict(int)  # çµ±è¨ˆå„ä¾†æºæ•¸é‡
    
    for i, item in enumerate(eval_data, 1):
        # è§£æå•ç­”
        question, expected_answer = extract_question_answer(item["text"])
        
        if not question or not expected_answer:
            print(f"âš ï¸  ç¬¬ {i} æ¢è³‡æ–™æ ¼å¼ç•°å¸¸ï¼Œè·³é")
            continue
        
        # ç”Ÿæˆå›ç­”
        prompt = format_qwen_input(question)
        inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **GENERATION_CONFIG,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)
        raw_answer = extract_model_response(full_output, prompt)

        # ============ ä¾†æºæª¢æ¸¬ ============
        final_answer, source_type, source_detail = detect_answer_source(
            question, raw_answer, kb_dict
        )
        source_stats[source_type] += 1
        # ===================================
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = calculate_similarity(expected_answer, final_answer)
        
        # è¨˜éŒ„çµæœ
        result = {
            "question": question,
            "expected": expected_answer,
            "model": final_answer,
            "similarity": similarity,
            "source": source_type,
            "source_detail": source_detail
        }
        results.append(result)
        
        # é¡¯ç¤ºç¯„ä¾‹ï¼ˆå‰ 5 æ¢ + æ¯ 10 æ¢ï¼‰
        if i <= 5 or i % 10 == 0:
            source_icon = {
                "RULE_OVERRIDE": "ğŸ”§",
                "KB_MATCH": "ğŸ“š",
                "MODEL_GENERATED": "ğŸ¤–"
            }
            print(f"\n[ç¯„ä¾‹ {i}] {source_icon.get(source_type, 'â“')} {source_type}")
            print(f"â“ å•é¡Œï¼š{question[:50]}...")
            print(f"âœ“ æœŸæœ›ï¼š{expected_answer[:60]}...")
            print(f"ğŸ’¬ æ¨¡å‹ï¼š{final_answer[:60]}...")
            print(f"ğŸ“Š ç›¸ä¼¼åº¦ï¼š{similarity:.1f}%")
            print(f"ğŸ” ä¾†æºï¼š{source_detail}")
    
    # 4. çµ±è¨ˆåˆ†æ
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ")
    print("=" * 70)
    
    similarities = [r["similarity"] for r in results]
    avg_similarity = sum(similarities) / len(similarities)
    
    print(f"\n### æ•´é«”è¡¨ç¾")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦ï¼š{avg_similarity:.2f}%")
    print(f"  æœ€é«˜ç›¸ä¼¼åº¦ï¼š{max(similarities):.2f}%")
    print(f"  æœ€ä½ç›¸ä¼¼åº¦ï¼š{min(similarities):.2f}%")
    
    # ä¾†æºåˆ†å¸ƒçµ±è¨ˆ
    print(f"\n### ç­”æ¡ˆä¾†æºåˆ†å¸ƒ")
    total = len(results)
    print(f"  ğŸ”§ è¦å‰‡ä¿®æ­£ï¼š{source_stats['RULE_OVERRIDE']} ({source_stats['RULE_OVERRIDE']/total*100:.1f}%)")
    print(f"  ğŸ“š çŸ¥è­˜åº«åŒ¹é…ï¼š{source_stats['KB_MATCH']} ({source_stats['KB_MATCH']/total*100:.1f}%)")
    print(f"  ğŸ¤– æ¨¡å‹ç”Ÿæˆï¼š{source_stats['MODEL_GENERATED']} ({source_stats['MODEL_GENERATED']/total*100:.1f}%)")
    
    # åˆ†ç´šçµ±è¨ˆ
    excellent = sum(1 for s in similarities if s >= 85)
    good = sum(1 for s in similarities if 70 <= s < 85)
    fair = sum(1 for s in similarities if 50 <= s < 70)
    poor = sum(1 for s in similarities if s < 50)
    
    print(f"\n### ç›¸ä¼¼åº¦åˆ†å¸ƒ")
    print(f"  å„ªç§€ (â‰¥85%)ï¼š{excellent} ({excellent/len(results)*100:.1f}%)")
    print(f"  è‰¯å¥½ (70-84%)ï¼š{good} ({good/len(results)*100:.1f}%)")
    print(f"  å°šå¯ (50-69%)ï¼š{fair} ({fair/len(results)*100:.1f}%)")
    print(f"  ä¸ä½³ (<50%)ï¼š{poor} ({poor/len(results)*100:.1f}%)")
    
    # å„ä¾†æºçš„å¹³å‡ç›¸ä¼¼åº¦
    print(f"\n### å„ä¾†æºå¹³å‡ç›¸ä¼¼åº¦")
    for source in ["RULE_OVERRIDE", "KB_MATCH", "MODEL_GENERATED"]:
        source_results = [r["similarity"] for r in results if r["source"] == source]
        if source_results:
            avg = sum(source_results) / len(source_results)
            print(f"  {source}: {avg:.2f}%")
    
    # 5. æœ€å·®æ¡ˆä¾‹åˆ†æ
    print(f"\n### âš ï¸  éœ€è¦æ”¹é€²çš„æ¡ˆä¾‹ (ç›¸ä¼¼åº¦ <50%)")
    worst_cases = sorted(results, key=lambda x: x["similarity"])[:5]
    for i, case in enumerate(worst_cases, 1):
        if case["similarity"] < 50:
            print(f"\n[æ¡ˆä¾‹ {i}] ç›¸ä¼¼åº¦ {case['similarity']:.1f}% | ä¾†æº: {case['source']}")
            print(f"  å•é¡Œï¼š{case['question'][:50]}...")
            print(f"  æœŸæœ›ï¼š{case['expected'][:60]}...")
            print(f"  å¯¦éš›ï¼š{case['model'][:60]}...")
            print(f"  ä¾†æºè©³æƒ…ï¼š{case['source_detail']}")
    
    # 6. ç›®æ¨™é”æˆæª¢æŸ¥
    print("\n" + "=" * 70)
    print("ğŸ¯ ç›®æ¨™é”æˆæª¢æŸ¥")
    print("=" * 70)
    
    if avg_similarity >= 85:
        print(f"âœ… å¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.2f}% â‰¥ 85% - é”æ¨™ï¼")
    elif avg_similarity >= 70:
        print(f"âš ï¸  å¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.2f}% - æ¥è¿‘ç›®æ¨™ï¼Œå»ºè­°å¾®èª¿")
    else:
        print(f"âŒ å¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.2f}% < 70% - éœ€è¦é‡æ–°è¨“ç·´")
        print("\nå»ºè­°èª¿æ•´ï¼š")
        print("  â€¢ å¢åŠ è¨“ç·´ epochs (å»ºè­° 10-15)")
        print("  â€¢ æé«˜ LoRA rank (å»ºè­° 24-32)")
        print("  â€¢ é™ä½ learning_rate (å»ºè­° 5e-5)")
    
    # 7. ä¿å­˜è©³ç´°çµæœ
    output_file = "./test_outputs/test_results_qwen_with_source.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "summary": {
                "avg_similarity": avg_similarity,
                "total_tests": len(results),
                "excellent_count": excellent,
                "good_count": good,
                "fair_count": fair,
                "poor_count": poor,
                "source_distribution": dict(source_stats)
            },
            "details": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜ï¼š{output_file}")

if __name__ == "__main__":
    main()
