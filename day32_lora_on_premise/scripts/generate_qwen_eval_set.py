#!/usr/bin/env python3
"""
Qwen2.5 è©•ä¼°é›†ç”Ÿæˆå™¨ v1.0 - å¾ BLOOM v4 æ”¹å¯«
ç­–ç•¥: å¾è¨“ç·´é›†çš„ 4 ç¨®é¢¨æ ¼ä¸­ï¼Œéš¨æ©ŸæŠ½å–æœªç”¨æ–¼è¨“ç·´çš„å•æ³•
æ ¼å¼: Qwen2.5-Instruct å°è©±æ ¼å¼

ä½¿ç”¨æ–¹å¼:
    python scripts/generate_qwen_eval_set.py
"""

import json
import random
from pathlib import Path

# ============================================
# è©•ä¼°é›†å•é¡Œ - èˆ‡ BLOOM ç‰ˆæœ¬ç›¸åŒ
# ============================================
EVAL_QUESTIONS = {
    "doc_611a2e74": [  # VPN - 5å€‹è©•ä¼°å•é¡Œ
        "vpn é€£ç·šé€¾æ™‚",
        "VPN é€£ä¸ä¸Šæ€éº¼è¾¦?",
        "æ–°äºº vpn è¨­å®š",
        "ç„¡æ³•å­˜å–å…¬å¸å…§éƒ¨ç¶²è·¯",
        "2025 vpn è¨­å®šæµç¨‹"
    ],
    
    "doc_30059bd8": [  # é ç«¯è¾¦å…¬
        "æ–°äººå¯ä»¥å±…å®¶è¾¦å…¬å—",
        "å‰›å…¥è·å¯ä»¥ç”³è«‹å±…å®¶è¾¦å…¬å—?",
        "è©¦ç”¨æœŸèƒ½é ç«¯å—",
        "è½‰æ­£å‰èƒ½ wfh å—",
        "é ç«¯è¾¦å…¬è³‡æ ¼"
    ],
    
    "doc_d8148fc3": [  # MFA
        "10æœˆå¾Œæ²’é–‹ mfa æœƒæ€æ¨£",
        "2025å¹´10æœˆå¾Œæ²’è¨­å®šé›™å› å­é©—è­‰æœƒç™¼ç”Ÿä»€éº¼?",
        "å¸³è™Ÿç™»ä¸é€²å»",
        "é›™å› å­é©—è­‰è¨­å®š",
        "å¼·åˆ¶å•Ÿç”¨ MFA çš„æ™‚é–“"
    ],
    
    "doc_2cc7937d": [  # å¯†ç¢¼é‡è¨­
        "å¯†ç¢¼å¿˜äº†å»å“ªé‡ç½®",
        "å…¬å¸å¸³è™Ÿå¯†ç¢¼å¿˜äº†è¦å»å“ªè£¡é‡ç½®?",
        "å¸³è™Ÿè¢«é–ä½",
        "é‡ç½®å¯†ç¢¼é é¢",
        "å“¡å·¥ç·¨è™Ÿå¿˜äº†æ€éº¼æŸ¥"
    ],
    
    "doc_f7379232": [  # éƒµä»¶é™„ä»¶
        "50MB ç°¡å ±æª”æ€éº¼å¯„",
        "æƒ³å¯„ 50MB çš„ç°¡å ±æª”çµ¦å®¢æˆ¶",
        "éƒµä»¶é™„ä»¶è¶…éä¸Šé™",
        "å¤§æª”æ¡ˆåˆ†äº«",
        "é›²ç«¯é€£çµæ€éº¼ç”¢ç”Ÿ"
    ],
    
    "doc_838f69ae": [  # å ±éŠ·
        "ç™¼ç¥¨è¶…éä¸€å€‹æœˆé‚„èƒ½å ±å—",
        "å‡ºå·®ç™¼ç¥¨è¶…éä¸€å€‹æœˆé‚„èƒ½å ±å¸³å—?",
        "å ±å¸³æœŸé™",
        "ç™¼ç¥¨éºå¤±",
        "é€¾æœŸå ±å¸³è™•ç†"
    ],
    
    "doc_37f6edeb": [  # å·®æ—…ç­‰ç´š
        "å‡ºå·®å¯ä»¥åå•†å‹™è»Šå»‚å—",
        "åœ‹å…§å‡ºå·®å¯ä»¥åå•†å‹™è»Šå»‚å—?",
        "é«˜éµå•†å‹™è‰™",
        "å‡ç­‰èª°æ‰¹",
        "åœ‹å¤–æ©Ÿç¥¨ç­‰ç´š"
    ],
    
    "doc_c7a8eb99": [  # è³‡å®‰å›å ±
        "æ”¶åˆ°å¥‡æ€ªéƒµä»¶è¦è·Ÿèª°èªª",
        "æ”¶åˆ°å¥‡æ€ªçš„éƒµä»¶è¦è·Ÿèª°èªª?",
        "é‡£é­šéƒµä»¶è™•ç†",
        "ç–‘ä¼¼è©é¨™å›å ±",
        "è³‡å®‰é€šå ±ç®¡é“"
    ],
    
    "doc_4941edf7": [  # MDM
        "æ–°äººç­†é›»è¨­å®š",
        "å‰›åˆ°è·çš„ç­†é›»è¦è¨­å®šä»€éº¼?",
        "mdm æ€éº¼è¨»å†Š",
        "ç£ç¢ŸåŠ å¯†",
        "7å¤©å…§å®Œæˆè¨­å®š"
    ],
    
    "doc_e4d9d352": [  # è³‡æ–™ä¿ç•™
        "å®¢æœå°è©±ä¿ç•™å¤šä¹…",
        "å®¢æœå°è©±æœƒä¿ç•™å¤šä¹…?",
        "èŠå¤©è¨˜éŒ„æœƒåˆªé™¤å—",
        "180å¤©å¾Œè³‡æ–™",
        "å¯©è¨ˆæ—¥èªŒä¿ç•™æœŸé™"
    ],
    
    "doc_51f8beab": [  # è«‹å‡
        "å¿˜è¨˜è«‹å‡å¯ä»¥è£œå—",
        "æ˜¨å¤©å¿˜è¨˜è«‹å‡ä»Šå¤©å¯ä»¥è£œå—?",
        "äº‹å¾Œè£œç™»å‡å–®",
        "è‡¨æ™‚è«‹å‡",
        "è«‹å‡è¦æå‰å—"
    ],
    
    "doc_2dfaeccd": [  # æœƒè­°å®¤
        "å¯ä»¥åŒæ™‚è¨‚å…©é–“æœƒè­°å®¤å—",
        "éƒ¨é–€æœƒè­°å¯ä»¥åŒæ™‚è¨‚å…©é–“æœƒè­°å®¤å—?",
        "æœƒè­°å®¤ä¸Šé™",
        "è¨‚3é–“æœƒè­°å®¤",
        "æœƒè­°å®¤å–æ¶ˆæµç¨‹"
    ],
    
    "doc_540013cd": [  # æª”æ¡ˆä¿å­˜
        "ç¨‹å¼ç¢¼ä¸€å®šè¦æ”¾é›²ç«¯å—",
        "é–‹ç™¼çš„ç¨‹å¼ç¢¼ä¸€å®šè¦æ”¾é›²ç«¯å—?",
        "é‡è¦æ–‡ä»¶å­˜æ”¾",
        "åªå­˜é›»è…¦å¯ä»¥å—",
        "é›²ç«¯ç¡¬ç¢Ÿåœ¨å“ª"
    ],
    
    "doc_e761823c": [  # åŠ ç­
        "é€±æœ«åŠ ç­è¦èª°æ‰¹",
        "å…­æ—¥è¦åŠ ç­éœ€è¦æ‰¾èª°æ ¸å‡†?",
        "å‡æ—¥åŠ ç­ç”³è«‹",
        "å¹³æ—¥åŠ ç­è¦ç”³è«‹å—",
        "é›™é‡æ ¸å‡†æ„æ€"
    ],
    
    "doc_2b808180": [  # è³‡å®‰åŸ¹è¨“
        "è³‡å®‰æ¸¬é©—æ²’éæœƒæ€æ¨£",
        "è³‡å®‰ç·šä¸Šæ¸¬é©—å¦‚æœæ²’é€šéæœƒæ€æ¨£?",
        "æ¯å¹´è¦è€ƒè³‡å®‰å—",
        "å¸³è™Ÿè¢«åœç”¨åŸå› ",
        "ç·šä¸Šæ¸¬é©—åœ¨å“ªè€ƒ"
    ]
}

# ============================================
# ç„¡ç­”æ¡ˆå•é¡Œ - 10å€‹è² æ¨£æœ¬
# ============================================
EVAL_UNANSWERABLE = [
    ("å…¬å¸æœ‰å“¡å·¥é¤å»³å—", "æŠ±æ­‰,çŸ¥è­˜åº«ä¸­æœªè¨˜è¼‰æ­¤è¦å®š,å»ºè­°å‘ç¸½å‹™éƒ¨é–€ç¢ºèªã€‚"),
    ("å¯ä»¥ç”³è«‹è‚²å¬°å‡å—", "æŠ±æ­‰,æ­¤è³‡è¨Šä¸åœ¨æˆ‘çš„çŸ¥è­˜ç¯„åœå…§,è«‹å‘äººè³‡éƒ¨é–€è©¢å•ã€‚"),
    ("éƒ¨é–€èšé¤è²»ç”¨å¯ä»¥å ±éŠ·å—", "æŠ±æ­‰,çŸ¥è­˜åº«æœªè¨˜è¼‰æ­¤è¦å®š,è«‹æ´½è²¡å‹™éƒ¨é–€ã€‚"),
    ("è½‰èª¿å…¶ä»–éƒ¨é–€çš„æµç¨‹", "æŠ±æ­‰,æ­¤è³‡è¨Šä¸åœ¨çŸ¥è­˜åº«ä¸­,è«‹å‘äººè³‡éƒ¨é–€è©¢å•ã€‚"),
    ("å…¬å¸æœ‰å“¡å·¥å®¿èˆå—", "æŠ±æ­‰,çŸ¥è­˜åº«æœªè¨˜è¼‰,å»ºè­°å‘è¡Œæ”¿éƒ¨é–€ç¢ºèªã€‚"),
    ("VPN å¯ä»¥åœ¨åœ‹å¤–ç”¨å—", "æŠ±æ­‰,çŸ¥è­˜åº«æœªè¨˜è¼‰æ­¤è¦å®š,è«‹æ´½ IT éƒ¨é–€ã€‚"),
    ("æœƒè­°å®¤å¯ä»¥å¤–å€Ÿå—", "æŠ±æ­‰,æ­¤è³‡è¨Šä¸åœ¨çŸ¥è­˜åº«ä¸­,è«‹æ´½ç¸½å‹™éƒ¨é–€ã€‚"),
    ("å…¬å¸æœ‰äº¤é€šè»Šå—", "æŠ±æ­‰,çŸ¥è­˜åº«æœªè¨˜è¼‰,å»ºè­°å‘è¡Œæ”¿éƒ¨é–€ç¢ºèªã€‚"),
    ("ç‰¹ä¼‘å¯ä»¥è³£æ‰å—", "æŠ±æ­‰,æ­¤è³‡è¨Šä¸åœ¨çŸ¥è­˜åº«ä¸­,è«‹å‘äººè³‡éƒ¨é–€è©¢å•ã€‚"),
    ("åŠ ç­å¯ä»¥æ›è£œä¼‘å—", "æŠ±æ­‰,çŸ¥è­˜åº«æœªè¨˜è¼‰æ­¤è¦å®š,è«‹æ´½äººè³‡éƒ¨é–€ã€‚")
]

def format_qwen_qa(question, answer):
    """
    æ ¼å¼åŒ–ç‚º Qwen2.5-Instruct å°è©±æ ¼å¼
    è¼¸å…¥: å•é¡Œã€ç­”æ¡ˆ
    è¼¸å‡º: <|im_start|>user\nå•é¡Œ<|im_end|>\n<|im_start|>assistant\nç­”æ¡ˆ<|im_end|>
    """
    return (
        f"<|im_start|>user\n{question}<|im_end|>\n"
        f"<|im_start|>assistant\n{answer}<|im_end|>"
    )

def load_kb(filepath="data/kb.jsonl"):
    """è¼‰å…¥çŸ¥è­˜åº«"""
    kb = []
    kb_path = Path(filepath)
    
    if not kb_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°çŸ¥è­˜åº«æª”æ¡ˆ: {filepath}")
    
    with open(kb_path, 'r', encoding='utf-8') as f:
        for line in f:
            kb.append(json.loads(line))
    
    return kb

def generate_eval_data(kb):
    """ç”Ÿæˆ Qwen æ ¼å¼è©•ä¼°è³‡æ–™"""
    eval_data = []
    
    # === 1. æœ‰ç­”æ¡ˆçš„å•é¡Œ ===
    for doc in kb:
        doc_id = doc['doc_id']
        if doc_id not in EVAL_QUESTIONS:
            continue
        
        questions = EVAL_QUESTIONS[doc_id]
        answer_text = f"ä¾æ“šå…¬å¸è¦ç¯„:{doc['text']}"
        
        for q in questions:
            formatted = format_qwen_qa(q, answer_text)
            eval_data.append({"text": formatted})
    
    # === 2. ç„¡ç­”æ¡ˆçš„å•é¡Œ ===
    for q, a in EVAL_UNANSWERABLE:
        formatted = format_qwen_qa(q, a)
        eval_data.append({"text": formatted})
    
    # ä¸æ‰“äº‚é †åºï¼Œä¿æŒçŸ¥è­˜é»åˆ†çµ„ï¼ˆæ–¹ä¾¿åˆ†æï¼‰
    
    return eval_data

def save_jsonl(data, filepath):
    """å„²å­˜ç‚º JSONL æ ¼å¼"""
    output_path = Path(filepath)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def analyze_coverage(eval_data, train_file="data/train_qwen_v1.jsonl"):
    """åˆ†æè©•ä¼°é›†èˆ‡è¨“ç·´é›†çš„é‡ç–Šåº¦"""
    # è¼‰å…¥è¨“ç·´é›†
    train_questions = set()
    try:
        with open(train_file, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)
                # æå–å•é¡Œéƒ¨åˆ† (åœ¨ <|im_start|>user å’Œ <|im_end|> ä¹‹é–“)
                text = item['text']
                if '<|im_start|>user\n' in text and '<|im_end|>' in text:
                    start = text.index('<|im_start|>user\n') + len('<|im_start|>user\n')
                    end = text.index('<|im_end|>', start)
                    question = text[start:end].strip()
                    train_questions.add(question)
    except FileNotFoundError:
        print(f"âš ï¸  æ‰¾ä¸åˆ°è¨“ç·´é›†æª”æ¡ˆ: {train_file}")
        return
    
    # åˆ†æè©•ä¼°é›†
    eval_questions = []
    for item in eval_data:
        text = item['text']
        if '<|im_start|>user\n' in text and '<|im_end|>' in text:
            start = text.index('<|im_start|>user\n') + len('<|im_start|>user\n')
            end = text.index('<|im_end|>', start)
            question = text[start:end].strip()
            eval_questions.append(question)
    
    # è¨ˆç®—é‡ç–Š
    overlap = sum(1 for q in eval_questions if q in train_questions)
    
    print(f"\nğŸ” é‡ç–Šåº¦åˆ†æ:")
    print(f"   è¨“ç·´é›†å•é¡Œæ•¸: {len(train_questions)}")
    print(f"   è©•ä¼°é›†å•é¡Œæ•¸: {len(eval_questions)}")
    print(f"   å®Œå…¨é‡ç–Š: {overlap} å€‹ ({overlap/len(eval_questions)*100:.1f}%)")
    print(f"   é¢¨æ ¼ç›¸ä¼¼: ~{len(eval_questions)-overlap} å€‹")
    
    if overlap > len(eval_questions) * 0.3:
        print(f"   âš ï¸  é‡ç–Šåº¦è¼ƒé«˜ï¼Œå¯èƒ½å°è‡´éæ“¬åˆ")
    else:
        print(f"   âœ… é‡ç–Šåº¦é©ä¸­ï¼Œèƒ½æ¸¬è©¦æ³›åŒ–èƒ½åŠ›")

def main():
    print("=" * 60)
    print("ğŸ¯ Qwen2.5 è©•ä¼°é›†ç”Ÿæˆå™¨ v1.0")
    print("=" * 60)
    
    # è¼‰å…¥çŸ¥è­˜åº«
    print("\nğŸ“š è¼‰å…¥çŸ¥è­˜åº«...")
    kb = load_kb("data/kb.jsonl")
    print(f"âœ… è¼‰å…¥ {len(kb)} æ¢è¦ç¯„")
    
    # ç”Ÿæˆè©•ä¼°è³‡æ–™
    print("\nğŸ”¨ ç”Ÿæˆè©•ä¼°è³‡æ–™...")
    eval_data = generate_eval_data(kb)
    
    # å„²å­˜
    output_file = "data/eval_qwen_v1.jsonl"
    save_jsonl(eval_data, output_file)
    
    print(f"\nâœ… è©•ä¼°é›†å·²å„²å­˜: {output_file}")
    print(f"   ç¸½ç­†æ•¸: {len(eval_data)}")
    
    # çµ±è¨ˆè³‡è¨Š
    has_answer = sum(1 for item in eval_data if "ä¾æ“šå…¬å¸è¦ç¯„" in item['text'])
    no_answer = len(eval_data) - has_answer
    
    print(f"\nğŸ“ˆ å¯¦éš›åˆ†å¸ƒ:")
    print(f"   æœ‰ç­”æ¡ˆ: {has_answer} ç­† ({has_answer/len(eval_data)*100:.1f}%)")
    print(f"   ç„¡ç­”æ¡ˆ: {no_answer} ç­† ({no_answer/len(eval_data)*100:.1f}%)")
    
    print(f"\nğŸ¨ æ ¼å¼ç‰¹é»:")
    print(f"   âœ… Qwen2.5-Instruct å°è©±æ ¼å¼")
    print(f"   âœ… ä½¿ç”¨ <|im_start|> / <|im_end|> æ¨™è¨˜")
    print(f"   âœ… ç§»é™¤ BLOOM çš„ <|endoftext|> æ¨™è¨˜")
    print(f"   âœ… æ¯å€‹çŸ¥è­˜é» 5 å€‹è®Šé«”")
    print(f"   âœ… ç¸½å…± {has_answer//5} å€‹çŸ¥è­˜é»")
    
    # é¡¯ç¤ºç¯„ä¾‹
    print(f"\nğŸ“ æ ¼å¼ç¯„ä¾‹:")
    sample = eval_data[0]['text']
    print(sample[:200] + "...")
    
    # åˆ†æé‡ç–Šåº¦
    analyze_coverage(eval_data)
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    main()