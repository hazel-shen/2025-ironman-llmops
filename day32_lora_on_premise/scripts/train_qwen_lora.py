"""
Qwen2.5-1.5B-Instruct LoRA å¾®èª¿è…³æœ¬ (M3 CPU å„ªåŒ–ç‰ˆ)
é‡å°ä¼æ¥­ Q&A çŸ¥è­˜åº«å„ªåŒ–
ç¡¬é«”ï¼šMacBook Air M3 24GB (CPU only)

åŸ·è¡Œæ–¹å¼ï¼š
  cd åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
  python scripts/train_qwen_lora.py
"""

import json
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
import os
from datetime import datetime

# ==================== é…ç½®å€ ====================
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
OUTPUT_DIR = "./qwen_lora_output"
TRAIN_FILE = "./data/train_qwen_final.jsonl" # 358
# TRAIN_FILE = "./data/train_qwen_v1.jsonl"
EVAL_FILE = "./data/eval_qwen_v1.jsonl"

# LoRA é…ç½®
LORA_CONFIG = {
    "r": 16,
    "lora_alpha": 32,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM"
}

# è¨“ç·´è¶…åƒæ•¸ï¼ˆM3 CPU å„ªåŒ–ï¼‰
TRAINING_CONFIG = {
    "learning_rate": 3e-4,           # æ›´é«˜å­¸ç¿’ç‡å¿«é€Ÿæ”¶æ–‚ (day32 ç‰ˆæœ¬ï¼š3e-4 / æœ€çµ‚èª¿æ•´ï¼š2e-4)
    "num_train_epochs": 3,           # åªè¦ 3 å€‹ epochs (day32 ç‰ˆæœ¬ï¼š3 / æœ€çµ‚èª¿æ•´ï¼š4)
    "per_device_train_batch_size": 4, # å¢å¤§ batch size
    "per_device_eval_batch_size": 4,
    "gradient_accumulation_steps": 2,
    "warmup_steps": 10,              # æœ€å° warmup (day32 ç‰ˆæœ¬ï¼š10 / æœ€çµ‚èª¿æ•´ï¼š30)
    "logging_steps": 5,
    "eval_strategy": "epoch",        # æ”¹æˆæ¯å€‹ epoch è©•ä¼°ä¸€æ¬¡
    "save_strategy": "epoch",        # æ¯å€‹ epoch ä¿å­˜ä¸€æ¬¡
    "max_grad_norm": 1.0,
    "lr_scheduler_type": "cosine",
    "weight_decay": 0.01,
    "fp16": False,
    "bf16": False,
    "save_total_limit": 1,
    "load_best_model_at_end": True,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": False,
    "dataloader_num_workers": 0,
}

MAX_LENGTH = 192 # è¦æ ¹æ“šå•ç­”çš„ token é•·åº¦èª¿æ•´

# ==================== è³‡æ–™è™•ç† ====================
def load_jsonl(filepath):
    """è¼‰å…¥ JSONL æ ¼å¼è³‡æ–™"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            data.append(item)
    print(f"âœ“ è¼‰å…¥ {len(data)} ç­†è³‡æ–™ï¼š{filepath}")
    return data

def preprocess_data(examples, tokenizer):
    """æ‰¹æ¬¡è™•ç†è³‡æ–™"""
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length",
        return_tensors="pt"
    )
    
    tokenized["labels"] = tokenized["input_ids"].clone()
    return tokenized

# ==================== ä¸»è¨“ç·´æµç¨‹ ====================
def main():
    print("=" * 60)
    print("ğŸš€ Qwen2.5-1.5B LoRA å¾®èª¿ (M3 CPU)")
    print("=" * 60)
    
    # 1. è¼‰å…¥è³‡æ–™
    print("\nğŸ“‚ è¼‰å…¥è³‡æ–™é›†...")
    train_data = load_jsonl(TRAIN_FILE)
    eval_data = load_jsonl(EVAL_FILE)
    
    train_dataset = Dataset.from_dict({"text": [item["text"] for item in train_data]})
    eval_dataset = Dataset.from_dict({"text": [item["text"] for item in eval_data]})
    
    print(f"âœ“ è¨“ç·´é›†ï¼š{len(train_dataset)} ç­†")
    print(f"âœ“ è©•ä¼°é›†ï¼š{len(eval_dataset)} ç­†")
    
    # 2. è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
    print(f"\nğŸ¤– è¼‰å…¥æ¨¡å‹ï¼š{MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=False,
        padding_side="right"
    )
    
    # å¦‚æœæ²’æœ‰ pad_tokenï¼Œå°±ç”¨ eos_token ä»£æ›¿ï¼špad_token å‘Šè¨´æ¨¡å‹ã€Œé€™æ˜¯å¡«å……ç‰©ï¼Œä¸è¦å­¸ç¿’ã€
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float32,
        device_map="cpu",
        trust_remote_code=False
    )
    
    print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    # 3. é…ç½® LoRAï¼ˆç§»é™¤ prepare_model_for_kbit_trainingï¼‰
    print("\nâš™ï¸ é…ç½® LoRA...")
    lora_config = LoraConfig(**LORA_CONFIG)
    model = get_peft_model(model, lora_config)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ å¯è¨“ç·´åƒæ•¸ï¼š{trainable_params:,} / {total_params:,} "
          f"({100 * trainable_params / total_params:.2f}%)")
    
    # 4. é è™•ç†è³‡æ–™
    print("\nğŸ“ é è™•ç†è³‡æ–™...")
    train_dataset = train_dataset.map(
        lambda x: preprocess_data(x, tokenizer),
        batched=True,
        remove_columns=["text"]
    )
    eval_dataset = eval_dataset.map(
        lambda x: preprocess_data(x, tokenizer),
        batched=True,
        remove_columns=["text"]
    )
    
    # 5. è¨“ç·´é…ç½®
    print("\nâš™ï¸ åˆå§‹åŒ–è¨“ç·´å™¨...")
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        **TRAINING_CONFIG,
        report_to=[]
    )
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # 6. é–‹å§‹è¨“ç·´
    print("\nğŸ”¥ é–‹å§‹è¨“ç·´...")
    total_steps = len(train_dataset) // (TRAINING_CONFIG['per_device_train_batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']) * TRAINING_CONFIG['num_train_epochs']
    print(f"é è¨ˆç¸½æ­¥æ•¸ï¼š{total_steps}")
    print(f"è©•ä¼°ç­–ç•¥ï¼š{TRAINING_CONFIG['eval_strategy']}")  # âœ… å‹•æ…‹é¡¯ç¤º
    print(f"âš ï¸  M3 CPU è¨“ç·´é ä¼°æ™‚é–“ï¼š35-45 åˆ†é˜")
    print(f"â° é–‹å§‹æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = datetime.now()
    trainer.train()
    end_time = datetime.now()
    
    # 7. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€çµ‚æ¨¡å‹...")
    final_dir = os.path.join(OUTPUT_DIR, "final_model")
    trainer.save_model(final_dir)
    tokenizer.save_pretrained(final_dir)
    
    # 8. è¨“ç·´ç¸½çµ
    print("\n" + "=" * 60)
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print("=" * 60)
    print(f"â±ï¸  è¨“ç·´æ™‚é–“ï¼š{end_time - start_time}")
    print(f"ğŸ“ æ¨¡å‹ä½ç½®ï¼š{final_dir}")
    print(f"ğŸ“Š æœ€ä½³ checkpointï¼š{trainer.state.best_model_checkpoint}")
    print(f"ğŸ“‰ æœ€ä½³ eval_lossï¼š{trainer.state.best_metric:.4f}")
    
    # 9. æœ€çµ‚è©•ä¼°
    print("\nğŸ“Š åŸ·è¡Œæœ€çµ‚è©•ä¼°...")
    eval_results = trainer.evaluate()
    print("è©•ä¼°æŒ‡æ¨™ï¼š")
    for key, value in eval_results.items():
        print(f"  {key}: {value:.4f}")
    
    print("\nğŸ¯ ç›®æ¨™æª¢æŸ¥ï¼š")
    final_loss = eval_results["eval_loss"]
    if final_loss < 0.6:
        print(f"  âœ… Eval loss {final_loss:.4f} < 0.6 - é”æ¨™ï¼")
    else:
        print(f"  âš ï¸  Eval loss {final_loss:.4f} >= 0.6 - å»ºè­°ç¹¼çºŒè¨“ç·´")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ python scripts/test_qwen_lora.py æ¸¬è©¦å¯¦éš›æ•ˆæœ")

if __name__ == "__main__":
    main()