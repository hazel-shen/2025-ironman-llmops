"""
è¨ºæ–· Qwen æ¨¡å‹åœ¨å“ªäº›å•é¡Œä¸Šç­”éŒ¯ï¼ˆè©•ä¼°é›† 85 æ¢å…¨éƒ¨è·‘éä¸€è¼ªï¼‰
æ‰¾å‡ºæ··æ·†çš„ä¸»é¡Œæ¨¡å¼
v2: åŠ å…¥é€²åº¦æ¢ + ä¿®æ­£è­¦å‘Š
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from difflib import SequenceMatcher
from tqdm import tqdm  # é€²åº¦æ¢

# é…ç½®
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
LORA_MODEL_PATH = "./qwen_lora_output/final_model"
EVAL_FILE = "./data/eval_qwen_v1.jsonl"

def load_model():
    """è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹"""
    print("ğŸ¤– è¼‰å…¥æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=False)
    
    # è¨­å®š pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float32,
        device_map="cpu"
    )
    model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
    model.eval()
    print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ\n")
    return model, tokenizer

def extract_answer(text):
    """æå– assistant çš„å›ç­”éƒ¨åˆ†"""
    if "<|im_start|>assistant" in text:
        answer = text.split("<|im_start|>assistant\n")[1].split("<|im_end|>")[0]
        return answer.strip()
    return text

def calculate_similarity(text1, text2):
    """è¨ˆç®—å…©æ®µæ–‡å­—çš„ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, text1, text2).ratio()

def test_model(model, tokenizer, test_data):
    """æ¸¬è©¦æ¨¡å‹ä¸¦åˆ†æéŒ¯èª¤"""
    errors = []
    correct = []
    
    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    for idx, item in enumerate(tqdm(test_data, desc="ğŸ§ª æ¸¬è©¦é€²åº¦", unit="é¡Œ")):
        full_text = item["text"]
        
        # æå–å•é¡Œå’Œæ­£ç¢ºç­”æ¡ˆ
        try:
            user_question = full_text.split("<|im_start|>user\n")[1].split("<|im_end|>")[0]
            correct_answer = extract_answer(full_text)
        except IndexError:
            print(f"âš ï¸  æ¨£æœ¬ {idx+1} æ ¼å¼éŒ¯èª¤ï¼Œè·³é")
            continue
        
        # æ§‹å»ºæ¸¬è©¦ prompt
        test_prompt = f"<|im_start|>user\n{user_question}<|im_end|>\n<|im_start|>assistant\n"
        
        # ç”Ÿæˆå›ç­”ï¼ˆä¿®æ­£è­¦å‘Šï¼‰
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        with torch.no_grad():  # ç¯€çœè¨˜æ†¶é«”
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=False,
                temperature=None,      # ç§»é™¤ temperature
                top_p=None,           # ç§»é™¤ top_p
                top_k=None,           # ç§»é™¤ top_k
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
        model_answer = extract_answer(generated_text)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = calculate_similarity(correct_answer, model_answer)
        
        result = {
            "idx": idx + 1,
            "question": user_question,
            "correct_answer": correct_answer,
            "model_answer": model_answer,
            "similarity": similarity * 100
        }
        
        if similarity < 0.9:  # ç›¸ä¼¼åº¦ < 90% è¦–ç‚ºéŒ¯èª¤
            errors.append(result)
        else:
            correct.append(result)
    
    return errors, correct

def analyze_errors(errors, correct):
    """åˆ†æéŒ¯èª¤æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¸¬è©¦çµæœç¸½è¦½")
    print("=" * 60)
    
    total = len(errors) + len(correct)
    print(f"âœ… æ­£ç¢ºï¼š{len(correct)} / {total} ({len(correct)/total*100:.1f}%)")
    print(f"âŒ éŒ¯èª¤ï¼š{len(errors)} / {total} ({len(errors)/total*100:.1f}%)")
    
    if not errors:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†ï¼")
        return {}
    
    print("\n" + "=" * 60)
    print("âŒ éŒ¯èª¤ç­”æ¡ˆè©³ç´°åˆ†æ")
    print("=" * 60)
    
    # æŒ‰ä¸»é¡Œåˆ†é¡éŒ¯èª¤
    topic_patterns = {
        "ç™»å…¥/å¸³è™Ÿ": ["ç™»å…¥", "å¸³è™Ÿ", "mfa", "å¯†ç¢¼", "é›™å› å­", "é©—è­‰", "ç™»ä¸é€²"],
        "VPN/ç¶²è·¯": ["vpn", "é€£ç·š", "ç¶²è·¯", "sso", "å…§ç¶²", "é ç«¯"],
        "æª”æ¡ˆ/é›²ç«¯": ["æª”æ¡ˆ", "é›²ç«¯", "å­˜æ”¾", "ä¸Šå‚³", "åˆ†äº«"],
        "è«‹å‡/åŠ ç­": ["è«‹å‡", "åŠ ç­", "å‡å–®", "è£œç™»"],
        "å ±éŠ·/å·®æ—…": ["å ±éŠ·", "ç™¼ç¥¨", "å‡ºå·®", "å·®æ—…", "å‡ç­‰"],
        "æœƒè­°å®¤": ["æœƒè­°å®¤", "é ç´„", "è¨‚"],
        "è£ç½®/MDM": ["mdm", "ç£ç¢Ÿ", "åŠ å¯†", "ç­†é›»", "è£ç½®"],
    }
    
    topic_errors = {topic: [] for topic in topic_patterns.keys()}
    topic_errors["å…¶ä»–"] = []
    
    for error in errors:
        question = error["question"].lower()
        correct = error["correct_answer"].lower()
        
        # åˆ¤æ–·å±¬æ–¼å“ªå€‹ä¸»é¡Œ
        matched_topic = "å…¶ä»–"
        for topic, keywords in topic_patterns.items():
            if any(kw in question or kw in correct for kw in keywords):
                matched_topic = topic
                break
        
        topic_errors[matched_topic].append(error)
    
    # è¼¸å‡ºåˆ†æçµæœ
    for topic, errs in topic_errors.items():
        if errs:
            print(f"\n### ğŸ“Œ ä¸»é¡Œï¼š{topic} ({len(errs)} å€‹éŒ¯èª¤)")
            for err in errs[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                print(f"\nå•é¡Œ {err['idx']}: {err['question']}")
                print(f"  âœ“ æ­£ç¢ºç­”æ¡ˆ: {err['correct_answer'][:60]}...")
                print(f"  âœ— æ¨¡å‹ç­”æ¡ˆ: {err['model_answer'][:60]}...")
                print(f"  ğŸ“Š ç›¸ä¼¼åº¦: {err['similarity']:.1f}%")
            
            if len(errs) > 5:
                print(f"  ... é‚„æœ‰ {len(errs)-5} å€‹éŒ¯èª¤")
    
    return topic_errors

def generate_confusion_matrix(errors):
    """ç”Ÿæˆæ··æ·†çŸ©é™£ï¼ˆå“ªå€‹ä¸»é¡Œè¢«èª¤åˆ¤æˆå“ªå€‹ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ”€ æ··æ·†æ¨¡å¼åˆ†æ")
    print("=" * 60)
    
    # å®šç¾©ä¸»é¡Œé—œéµå­—
    topics = {
        "VPN": ["vpn", "é€£ç·š", "sso"],
        "MFA": ["mfa", "é›™å› å­", "å¤šå› å­", "é©—è­‰", "2025-10-01"],
        "å¯†ç¢¼": ["å¯†ç¢¼", "é‡è¨­", "/id/reset", "å“¡å·¥ç·¨è™Ÿ"],
        "å ±éŠ·": ["å ±éŠ·", "ç™¼ç¥¨", "30 å¤©"],
        "é ç«¯": ["é ç«¯", "å±…å®¶", "wfh", "è©¦ç”¨æœŸ"],
    }
    
    confusion = {}
    
    for error in errors:
        correct_ans = error["correct_answer"]
        model_ans = error["model_answer"]
        
        # åˆ¤æ–·æ­£ç¢ºç­”æ¡ˆå±¬æ–¼å“ªå€‹ä¸»é¡Œ
        correct_topic = "å…¶ä»–"
        for topic, keywords in topics.items():
            if any(kw in correct_ans.lower() for kw in keywords):
                correct_topic = topic
                break
        
        # åˆ¤æ–·æ¨¡å‹ç­”æ¡ˆå±¬æ–¼å“ªå€‹ä¸»é¡Œ
        model_topic = "å…¶ä»–"
        for topic, keywords in topics.items():
            if any(kw in model_ans.lower() for kw in keywords):
                model_topic = topic
                break
        
        # è¨˜éŒ„æ··æ·†
        if correct_topic != model_topic:
            key = f"{correct_topic} â†’ {model_topic}"
            if key not in confusion:
                confusion[key] = []
            confusion[key].append(error)
    
    if confusion:
        print("\nå¸¸è¦‹çš„æ··æ·†é¡å‹ï¼š")
        for pattern, errs in sorted(confusion.items(), key=lambda x: len(x[1]), reverse=True):
            print(f"  â€¢ {pattern}: {len(errs)} æ¬¡")
            if len(errs) > 0:
                print(f"    ç¯„ä¾‹å•é¡Œï¼š{errs[0]['question']}")
    else:
        print("æœªç™¼ç¾æ˜é¡¯çš„æ··æ·†æ¨¡å¼")

def main():
    # è¼‰å…¥è©•ä¼°è³‡æ–™
    with open(EVAL_FILE, 'r', encoding='utf-8') as f:
        eval_data = [json.loads(line) for line in f]
    
    print(f"ğŸ“‚ è¼‰å…¥è©•ä¼°è³‡æ–™ï¼š{len(eval_data)} ç­†\n")
    
    # è¼‰å…¥æ¨¡å‹
    model, tokenizer = load_model()
    
    # æ¸¬è©¦æ¨¡å‹
    errors, correct = test_model(model, tokenizer, eval_data)
    
    # åˆ†æéŒ¯èª¤
    topic_errors = analyze_errors(errors, correct)
    
    # æ··æ·†çŸ©é™£
    if errors:
        generate_confusion_matrix(errors)
    
    # ä¿å­˜çµæœ
    output = {
        "total": len(eval_data),
        "correct": len(correct),
        "errors": len(errors),
        "error_rate": len(errors) / len(eval_data),
        "accuracy": len(correct) / len(eval_data),
        "detailed_errors": errors,
        "detailed_correct": correct[:10],  # åªä¿å­˜å‰ 10 å€‹æ­£ç¢ºç­”æ¡ˆ
        "topic_errors": {k: len(v) for k, v in topic_errors.items() if v}
    }
    
    with open("error_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“Š æ­£ç¢ºç‡ï¼š{len(correct) / len(eval_data) * 100:.1f}%")
    print(f"ğŸ“ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³ï¼šerror_analysis.json")
    
    # çµ¦å‡ºå»ºè­°
    if len(errors) > 0:
        print("\nğŸ’¡ æ”¹å–„å»ºè­°ï¼š")
        if any("ç™»å…¥" in k or "å¸³è™Ÿ" in k for k in topic_errors.keys()):
            print("  â€¢ ç™»å…¥/å¸³è™Ÿç›¸é—œä¸»é¡Œæ··æ·†åš´é‡ï¼Œå»ºè­°å¢åŠ å€åˆ†æ€§æ¨£æœ¬")
        if len(errors) / len(eval_data) > 0.3:
            print("  â€¢ éŒ¯èª¤ç‡ >30%ï¼Œå»ºè­°åŸ·è¡Œ enhance_data.py å¢å¼·è¨“ç·´è³‡æ–™")
        elif len(errors) / len(eval_data) > 0.15:
            print("  â€¢ éŒ¯èª¤ç‡ 15-30%ï¼Œå¯è€ƒæ…®å¾®èª¿è¶…åƒæ•¸æˆ–å°å¹…å¢åŠ è³‡æ–™")
        else:
            print("  â€¢ éŒ¯èª¤ç‡ <15%ï¼Œæ¨¡å‹å·²ç›¸ç•¶ä¸éŒ¯ï¼")

if __name__ == "__main__":
    main()